---
title: "Activity Recognition for Smart Manufacturing"
collection: research
type: "Research"
permalink: /research/research-project-2
layout: single
slug: "research-project-2"
image_width: "200px"  # Custom width for this post's image
excerpt: "<div>
    <p>This project focuses on activity recognition in smart manufacturing environments, leveraging YOLO, RNN, and computer vision-based systems to develop real-time, human-centered manufacturing processes. The research spans multiple areas, including human-robot collaboration, action recognition, and multi-modal learning, culminating in highly accurate prediction models and intelligent control systems.</p>
    <div style='text-align: center;'>
            <img src='https://haodongchen.github.io/haodongchen.github.io/images/research-project-activity-recognition.jpg' alt='Example Image' width='200' />
    </div>
</div>"
---

<div style="text-align: center;">
    <img src="https://yourwebsite.com/images/research-project2-activity-recognition-background.jpg" alt="Example Image" width="800" />
</div>

## Goals
- Develop a YOLO-driven system for real-time activity recognition and predictive control.
- Utilize RNNs, CNN-LSTM, and multi-modal models for spatial and temporal feature recognition.
- Enhance human-robot collaboration with gesture and speech control.

<div style="text-align: center;">
    <img src="https://yourwebsite.com/images/research-project-activity-recognition-goal.jpg" alt="Example Image" width="700" />
</div>

## Key Contributions
- **YOLO-driven Wound Tissue Detection:** Initiated a proposal for medical device software using YOLO for wound tissue detection, coupled with Generative Adversarial Networks (GANs) for data augmentation.
- **Human-Computer Interaction (HCI) Assistance:** Designed an eye gaze-based HCI assembly assistance software using gaze estimation, PyQt, OpenCV, and Matplotlib.
- **Feature Recognition in Fine-Grained Actions:** Created spatial and temporal feature recognition of fine-grained actions using computer vision, signal processing, and RNN models (LSTM and GRU) with CUDA/GPU programming.
<div style="text-align: center;">
    <img src="https://yourwebsite.com/images/research-project-activity-recognition-contribution.jpg" alt="Example Image" width="900" />
</div>
- **Real-Time Assembly Recognition:** Achieved over 95% accuracy in assembly step recognition using a dual-zone CNN-LSTM model and PyTorch.
- **Multi-Threaded Human-Robot Collaboration (HRC):** Designed real-time multi-threaded HRC software integrating gestures and speech commands using Python, Raspberry Pi, and ROS, managing seven parallel tasks.
- **3D Pose Action Dataset:** Constructed and analyzed a 3D skeleton/pose action dataset of human subjects using Python, Graph Neural Networks, and Pandas.
- **Multi-Modal Learning and Transfer Learning:** Modeled multi-modal training datasets (video, image, etc.) for training and validation, achieving 95% accuracy for action recognition using transfer learning with VGG, ResNet, and Inception models.
- **Real-Time Gesture and NLP Recognition:** Attained 98% accuracy in gesture recognition and 95% in natural language processing (NLP) under noisy conditions using CNNs and large language models (LLMs).
- **Instance Segmentation of Tools and Parts:** Achieved 95% accuracy on instance segmentation of tools and parts using Mask RCNN with an object-oriented Python implementation.

<div style="text-align: center;">
    <img src="https://yourwebsite.com/images/research-project-activity-recognition-results.jpg" alt="Example Image" width="1300" />
</div>

## Technologies Utilized
- **Computer Vision Models:** YOLO, Mask RCNN, VGG, ResNet, CNN, LSTM, GRU, GANs.
- **Programming and Tools:** Python, PyTorch, TensorFlow, OpenCV, Matplotlib, PyQt, ROS, Raspberry Pi.
- **Hardware and Frameworks:** CUDA/GPU programming, multi-threaded processing, object-oriented software design.
<div style="text-align: center;">
    <img src="https://yourwebsite.com/images/research-project-activity-recognition-technology.jpg" alt="Example Image" width="1400" />
</div>

## Impact
This project has advanced the field of smart manufacturing by enabling real-time, human-centered assembly recognition and prediction. The combination of gesture, pose, and NLP-based collaboration with robots facilitates more efficient and safer human-robot interactions in industrial settings.

---

If you'd like me to adjust this further or customize any specific section, please let me know!
